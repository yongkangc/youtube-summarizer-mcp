{
  "memory_architecture": {
    "comparison": [
      {
        "chip": "Nvidia H100 SXM",
        "memory_type": "HBM3",
        "capacity_gb": 80,
        "bandwidth_tb_s": 3.35,
        "power_tdp_w": 700,
        "process_node": "TSMC 4N",
        "die_size_mm2": 814,
        "peak_fp8_pflops": 3.96
      },
      {
        "chip": "Nvidia H200",
        "memory_type": "HBM3e",
        "capacity_gb": 141,
        "bandwidth_tb_s": 4.8,
        "power_tdp_w": 700,
        "process_node": "TSMC 4N",
        "die_size_mm2": 814,
        "peak_fp8_pflops": 3.96
      },
      {
        "chip": "Groq LPU",
        "memory_type": "SRAM",
        "capacity_mb": 230,
        "bandwidth_tb_s": 80,
        "power_tdp_w": 300,
        "power_range_w": [240, 375],
        "process_node": "GlobalFoundries 14nm",
        "die_size_mm2": 725,
        "architecture": "Tensor Streaming Processor (TSP)"
      },
      {
        "chip": "Cerebras WSE-3",
        "memory_type": "SRAM",
        "capacity_gb": 44,
        "bandwidth_pb_s": 21,
        "power_system_kw": 25,
        "power_range_kw": [23, 27],
        "process_node": "TSMC 5nm",
        "die_size_mm2": 46225,
        "cores": 900000,
        "note": "Wafer-scale processor"
      },
      {
        "chip": "AMD MI300X",
        "memory_type": "HBM3",
        "capacity_gb": 192,
        "bandwidth_tb_s": 5.3,
        "power_tdp_w": 750,
        "process_node": "TSMC 5nm + 6nm",
        "price_usd": [10000, 15000]
      },
      {
        "chip": "AMD MI400",
        "memory_type": "HBM4",
        "capacity_gb": 432,
        "bandwidth_tb_s": 19.6,
        "timeline": "2026",
        "peak_fp4_pflops": 40
      }
    ],
    "bandwidth_ratios": {
      "groq_vs_h100": 23.88,
      "cerebras_vs_h100": 6268.66,
      "description": "Groq has 24x H100 bandwidth; Cerebras has 6,270x"
    }
  },
  "inference_performance": {
    "benchmarks": [
      {
        "hardware": "Groq 576-LPU cluster",
        "model": "Llama 3.3 70B",
        "tokens_per_sec": 1660,
        "ttft_sec": [0.2, 0.3],
        "mode": "speculative decoding",
        "source": "Groq benchmarks"
      },
      {
        "hardware": "Groq 576-LPU cluster",
        "model": "Llama 3 70B",
        "tokens_per_sec": [280, 300],
        "ttft_sec": [0.2, 0.3],
        "mode": "standard decode",
        "source": "Groq benchmarks"
      },
      {
        "hardware": "Cerebras WSE-3",
        "model": "Llama 3.1 70B",
        "tokens_per_sec": 2100,
        "note": "8x faster than H200",
        "source": "Cerebras benchmarks"
      },
      {
        "hardware": "Cerebras WSE-3",
        "model": "Llama 4 Maverick",
        "tokens_per_sec": 2500,
        "source": "Cerebras benchmarks"
      },
      {
        "hardware": "8x Nvidia H100",
        "model": "Llama 2 70B",
        "tokens_per_sec": 24323,
        "ttft_ms": [10, 100],
        "mode": "MLPerf offline batch",
        "source": "MLPerf benchmarks"
      },
      {
        "hardware": "8x AMD MI300X",
        "model": "Llama 2 70B",
        "tokens_per_sec": 23512,
        "note": "Within 3% of H100",
        "source": "MLPerf benchmarks"
      }
    ],
    "notes": {
      "batch_vs_realtime": "H100 numbers are batch throughput; SRAM designs excel at real-time latency",
      "decode_equation": "time_per_token = model_bytes / memory_bandwidth"
    }
  },
  "kv_cache_requirements": {
    "formula": "KV_cache (bytes) = 2 * layers * kv_heads * head_dim * seq_len * batch * precision",
    "examples": [
      {
        "model": "Llama 3 8B",
        "context_4k_gb": 0.5,
        "context_32k_gb": 4.0,
        "context_128k_gb": 15.6
      },
      {
        "model": "Llama 3 70B",
        "context_4k_gb": 1.3,
        "context_32k_gb": 10.4,
        "context_128k_gb": 40.0
      },
      {
        "model": "Llama 3.1 405B",
        "context_4k_gb": 2.5,
        "context_32k_gb": 20.0,
        "context_128k_gb": 66.0
      }
    ],
    "groq_implications": {
      "chips_for_kv_cache_70b_128k": 174,
      "chips_for_model_weights_70b": 600,
      "calculation": "40GB / 230MB = 174 chips for KV cache alone"
    },
    "mitigation_techniques": [
      "FP8 quantization (halves cache size)",
      "PagedAttention (2-4x throughput gains)",
      "Grouped-Query Attention (4-8x KV reduction)"
    ]
  },
  "multi_chip_scaling": {
    "groq_configurations": [
      {
        "model_size": "7-8B",
        "chips_required": 32,
        "configuration": "4 servers",
        "economics": "Most economical"
      },
      {
        "model_size": "70B",
        "chips_required": 576,
        "configuration": "8 racks x 9 servers x 8 chips",
        "economics": "Infrastructure-heavy"
      },
      {
        "model_size": "405B",
        "chips_required": 3500,
        "configuration": "Multiple data center sections",
        "economics": "Prohibitive"
      }
    ],
    "interconnect": {
      "groq": "Plesiosynchronous protocol, fiber optic between all chips",
      "cerebras": "SwarmX interconnect, 2048 systems for 256 exaFLOPS"
    }
  },
  "energy_efficiency": {
    "joules_per_token": {
      "groq_lpu": [1, 3],
      "nvidia_gpu": [10, 30]
    },
    "efficiency_ratio": "10x advantage for Groq on decode workloads",
    "source": "Groq claims"
  },
  "sources": [
    "https://groq.com/lpu-architecture",
    "https://www.baseten.co/blog/llm-transformer-inference-guide/",
    "MLPerf Inference benchmarks"
  ]
}
