## Source
**Source**: https://youtu.be/PqVbypvxDto

---

## Summary

This is an episode of "Google DeepMind: The Podcast" featuring host Professor Hannah Fry interviewing Demis Hassabis, CEO and co-founder of Google DeepMind. The conversation covers the current state of AI development, the path toward AGI (Artificial General Intelligence), and the profound societal implications of these technologies. Hassabis shares insights on everything from world models and simulations to the philosophical question of what distinguishes human consciousness from computation, while reflecting on the enormous responsibility of leading one of the world's most advanced AI research organizations during this pivotal moment in history.

---

## Key Insights

### Current State of AI Development
- AI has progressed enormously in the past year—feels like "10 years packed into one year"
- Gemini 3 has been released with significant improvements in multimodal capabilities
- World models advancement over the summer is a particularly exciting development
- No real "scaling wall" has been encountered—there are diminishing returns but still significant improvements worth the investment
- 50% of DeepMind's effort goes toward scaling, 50% toward innovation—Hassabis believes both are needed for AGI

### Root Node Problems & Scientific Applications
- AlphaFold approaching 5-year anniversary—proof that AI can solve "root node" problems that unlock downstream benefits
- Material science is next frontier—room temperature superconductors and better batteries are "on the cards"
- Fusion energy partnership with Commonwealth Fusion deepened—AI helping contain plasma and potentially design materials
- Quantum computing collaboration—using ML to help with error correction codes
- These "root node" solutions (like fusion) would transform multiple domains: energy, climate, water access through desalination, even rocket fuel production

### The Math Paradox
- AI can win gold medals at International Maths Olympiad on super hard problems
- Yet makes trivial mistakes on basic logic problems and can't play decent chess
- Called "jagged intelligences"—PhD level in some dimensions, below high school in others
- This inconsistency is one of the key things missing before AGI
- Causes include how images are tokenized (may not see individual letters) and insufficient thinking/reasoning time usage

### Hallucinations and Confidence
- Models still sometimes force themselves to answer when they shouldn't, leading to hallucinations
- Better models are getting better at knowing what they know (introspection)
- Need confidence scores similar to AlphaFold's approach
- Current systems are like "talking to a person on a bad day who tells you the first thing that comes to mind"
- Systems need to pause, think, and go back over what they were about to say

### World Models and Simulation
- World models represent Hassabis's "longest-standing passion"
- Language models understand more about the world than expected, but spatial dynamics and physical context remain challenging
- Senses (motor angles, smell) are difficult to describe in language
- World models are essential for robotics and universal assistants
- Genie (interactive world model) and Veo (video model) demonstrate understanding of world mechanics
- Test: if you can generate realistic worlds, you must have encapsulated the mechanics

### SIMA and Agent Training
- SIMA (Simulated Agents) 2.0 released—agents with Gemini under the hood that can be instructed in natural language
- Combining Genie + SIMA: dropping a SIMA agent into a Genie-generated world creates AI-to-AI interaction
- Could enable infinite training examples—whatever SIMA tries to learn, Genie can create on the fly
- Potential for automatic generation of millions of increasingly difficult tasks
- Applications: game companions, robotics training, "end of boring NPCs"

### Physics Accuracy in Simulations
- Current video models look realistic but aren't physics-accurate enough for robotics
- Creating physics benchmarks using game engines (which have accurate physics)
- Testing against Newton's laws of motion with simple experiments (balls rolling, pendulums)
- Genie and Veo are "approximations"—reflections and liquids look amazing but need to go beyond amateur perception
- Goal: hold up to proper physics-grade experimentation

### Missing Pieces for AGI
- Consistency across domains (closing the jagged intelligence gaps)
- Reliable reasoning and thinking (systems don't always use thinking time usefully)
- Online/continual learning (current systems don't learn after deployment)
- Convergence of different projects (Gemini, Genie, SIMA) into one unified model

### The AI Bubble Question
- Parts of AI ecosystem are probably in bubbles (seed rounds at tens of billions for unproven startups)
- Big tech valuations have real business underlying them—remains to be seen
- Natural overcorrection: from "no one believed in AI" 15 years ago to "only thing people talk about in business"
- Similar patterns seen with internet and mobile
- Google DeepMind well-positioned either way due to:
  - Own stack (TPUs)
  - Existing profitable products to integrate AI into (Search, Workspace, YouTube, Chrome)
  - New products like Gemini app

### Avoiding Social Media's Mistakes
- Critical not to build AI that maximizes user engagement
- Risk of echo chambers of one through overly sycophantic systems
- Gemini 3 persona: warm, helpful, light, but succinct and willing to push back
- Developing a "science of personality and persona"—measuring authenticity, humor, etc.
- Base personality adhering to scientific method, with personalization layer on top

### Industrial Revolution Lessons
- Studying the Industrial Revolution for lessons on managing societal transformation
- IR brought benefits: reduced child mortality, modern medicine, sanitary conditions
- Also brought dislocations requiring ~century to work out, creation of unions, etc.
- Key difference: AI transformation will be "10X bigger and 10X faster" (decade, not century)

### Post-AGI Economics and Society
- Current economic system (labor for resources) won't function the same way
- Need new economic models to ensure benefits are widely distributed
- Universal basic income may be part of the solution but not complete
- Exploring ideas like direct democracy with voting credits for community decisions
- Philosophical questions: If jobs change and we're post-scarcity, what happens to purpose?
- Many get purpose from jobs and providing for families—this needs addressing

### International Collaboration Concerns
- Surprisingly little discussion happening given 5-10 year AGI timelines
- Existing institutions seem fragmented and not influential enough
- Geopolitical tensions making cooperation harder than ever
- Climate change shows how difficult international agreement is
- Hope: as stakes get higher and systems more powerful, governments may "see sense"
- Possible that an incident (hopefully "medium-sized") becomes a warning shot prompting collaboration

### The Turing Machine Question
- Central question of Hassabis's life: what is the limit of a Turing machine?
- Everything DeepMind does pushes this limit (protein folding, Go, etc.)
- Nobody has found anything in the universe that's non-computable so far
- If consciousness involves quantum effects (as Roger Penrose believes), classical machines may never have it
- If consciousness is classical computation, there may be no limit—Turing machines might model everything in the universe
- Hassabis's current working assumption: everything is computationally tractable

### Information as Fundamental
- Reality is a construct of the mind (Kantian philosophy)
- All sensory experiences (warmth, touch) are ultimately information processing
- Biology is an information-processing system—this view will help cure all diseases
- Working on physics theories with information as the most fundamental unit (not energy, not matter)

---

## Main Arguments or Thesis

1. **AGI requires both scaling AND innovation**: Unlike some who focus on one or the other, Hassabis believes reaching AGI will require continued progress on both fronts, with Google DeepMind investing roughly 50/50.

2. **The "jagged intelligence" problem is key**: Current AI systems' inconsistency—brilliant at some tasks, poor at others—is a fundamental barrier to AGI that must be solved.

3. **World models are essential for true understanding**: Language models have gone surprisingly far, but spatial dynamics, physical intuition, and embodied knowledge require world models and simulation.

4. **Society is dramatically underprepared**: Despite 5-10 year AGI timelines, there's insufficient institutional, economic, and international preparation for the transformation coming.

5. **Everything may be computable**: Hassabis's working hypothesis is that there's no limit to what Turing machines can model, with consciousness potentially being classical computation unless proven otherwise.

---

## Notable Quotes or Highlights

**On scaling progress**:
> "Nobody's found anything in the universe that's non-computable, so far."

**On AI's current limitations**:
> "Most of the time, that will be OK. But then sometimes, when it's a very difficult thing, you'd want to stop, pause for a moment, and maybe go over what you were about to say and adjust what you were about to say."

**On the pace of change**:
> "The difference this time is that it's probably going to be 10X bigger than the Industrial Revolution, and it will probably happen 10X faster, so more like a decade, unfold over a decade, than a century."

**On his life's mission**:
> "My mission has always been to help the world steward AGI safely over the line for all of humanity."

**On Turing machines**:
> "What is the limit of a Turing machine? I think that's the central question in my life, really, ever since I found out about Turing and Turing machines."

**On the emotional weight**:
> "I don't sleep very much, partly because there's too much work, but also I have trouble sleeping."

**On competition**:
> "Investor friends of mine and VC friends of mine who were around in the dotcom era say this is 10X more ferocious and intense than that was."

**On AI personas**:
> "I don't think that's good in general for society if that were to happen"—referring to AI reinforcing false beliefs like flat earth theories.

---

## Practical Takeaways

1. **For researchers**: Consistency and reliability across domains is a key research priority—solving the "jagged intelligence" problem is essential for AGI.

2. **For policymakers**: The window for developing international frameworks and new economic models is short (5-10 years)—preparation should begin now.

3. **For business leaders**: Position for both scenarios—continued AI acceleration or potential retrenchment/bubble burst.

4. **For society**: Start thinking about purpose beyond work—as traditional labor-for-resources economics may fundamentally change.

5. **For AI users**: Expect confidence scores and better introspection from AI systems in the future; current systems still benefit from human verification.

---

## Additional Context

- **Google DeepMind's positioning**: Has TPUs, profitable products to integrate AI into, deep research bench—well-positioned regardless of market conditions
- **Historical context**: DeepMind started ~15 years ago when "no one believed in AI"—the overcorrection to current hype is natural
- **Philosophical influences**: Hassabis cites Kant (reality as construct of mind) and Spinoza as favorite philosophers
- **Personal background**: Chess player, game designer—trained his whole life for this moment of leading frontier AI development
- **AlphaGo to AlphaZero progression**: Current LLMs are more like AlphaGo (learning from human knowledge); AlphaZero-style self-discovery may come next
- **Isomorphic Labs**: Hassabis's biotech company applying the information-processing view to cure diseases
